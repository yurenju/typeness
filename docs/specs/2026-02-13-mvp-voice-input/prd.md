# Typeness MVP：語音輸入核心驗證

## 1. 簡介

Typeness 是一款本地端語音輸入工具，將語音轉為經過整理的書面文字。本 MVP 的目的是用**最小的範圍**驗證三個核心能力：

1. 語音辨識（Whisper）能否在本地端即時產出繁體中文結果
2. LLM 能否有效移除口語贅字並保持原意
3. LLM 能否將口語內容自動格式化為結構化文字（列表、段落、標點）

本 MVP 是一個**單一 Python 腳本**，透過終端機互動（Enter 鍵控制錄音），結果直接印在終端機上。不涉及剪貼簿、系統匣、快捷鍵等系統整合。

基於 `docs/research/2026-02-12-voice-input-tool-feasibility.md` 的研究發現。

---

## 2. 目標

- **驗證語音辨識品質**：確認 Whisper large-v3-turbo 在繁體中文上的辨識準確度是否符合日常使用需求
- **驗證 LLM 贅字清理效果**：確認本地 LLM 能否在不改變語意的前提下，有效移除口語中的贅字與重複
- **驗證 LLM 格式化能力**：確認本地 LLM 能否將口語內容自動轉換為列表、段落等結構化格式
- **驗證端到端延遲**：確認從停止錄音到文字輸出的總延遲是否在可接受範圍（目標 < 3 秒）
- **建立開發環境**：使用 uv 初始化專案，確認所有依賴能順利安裝和運行

---

## 3. 使用者故事

### 故事 1：基本語音輸入

> 作為使用者，我想在終端機中按下 Enter 開始錄音、說一段話、再按 Enter 結束，然後看到辨識結果印在畫面上，這樣我可以確認語音辨識的品質。

### 故事 2：贅字自動清理

> 作為使用者，我說話時難免會有「嗯」、「那個」、「就是說」等贅字和重複，我希望系統能自動清除這些內容，讓輸出讀起來像是書面文字，同時不改變我要表達的意思。

### 故事 3：自動格式化為列表

> 作為使用者，當我口述「第一個是蘋果，第二個是香蕉，第三個是橘子」時，我希望輸出自動變成：
> ```
> 1. 蘋果
> 2. 香蕉
> 3. 橘子
> ```

### 故事 4：自動段落分割

> 作為使用者，當我在一段錄音中談論了多個主題時，我希望輸出能在適當的位置自動分段，而不是一整段文字擠在一起。

### 故事 5：標點修正

> 作為使用者，我希望輸出的標點符號是正確的，即使我在說話時沒有特別強調標點的位置。

---

## 4. 功能需求

### 4.1 專案初始化

- **FR-1**：使用 uv 初始化 Python 專案，管理所有依賴
- **FR-2**：提供清楚的安裝指令，讓使用者能一鍵執行腳本

### 4.2 錄音控制

- **FR-3**：啟動腳本後，顯示提示訊息等待使用者操作
- **FR-4**：使用者按下 Enter 鍵開始錄音，終端機顯示「錄音中...」提示
- **FR-5**：使用者再按一次 Enter 鍵結束錄音
- **FR-6**：錄音格式為 16kHz 單聲道，直接符合 Whisper 輸入需求

### 4.3 語音辨識

- **FR-7**：使用 Whisper large-v3-turbo 模型進行本地語音辨識
- **FR-8**：辨識語言設定為中文（zh），並引導輸出繁體中文
- **FR-9**：首次執行時自動下載模型

### 4.4 文字後處理（LLM）

- **FR-10**：使用本地 LLM 進行文字後處理，不需要額外的外部服務
- **FR-11**：LLM 處理需涵蓋以下內容：
  - 移除口語贅字（嗯、啊、那個、就是、然後、對對對等）
  - 移除因停頓造成的重複內容
  - 修正標點符號
  - 將口述的列舉內容轉為編號列表
  - 在主題轉換處加入段落分隔
- **FR-12**：LLM 處理時必須保持說話者的原始語意和用詞不變，禁止添加原文沒有的內容

### 4.5 結果輸出

- **FR-13**：在終端機中分別顯示：
  - **Whisper 原始辨識結果**：讓使用者對照辨識品質
  - **LLM 處理後結果**：最終的整理結果
  - **處理時間統計**：各階段耗時（錄音時長、Whisper 辨識時間、LLM 處理時間、總延遲）
- **FR-14**：處理完成後回到等待狀態，使用者可繼續下一次錄音，或按 Ctrl+C 結束程式

---

## 5. 非目標（超出範圍）

本 MVP **不包含**以下功能：

- 剪貼簿整合與自動貼上
- 全域快捷鍵監聽（如左 Alt）
- 系統匣圖示與背景服務
- 規則引擎處理（本 MVP 全部由 LLM 處理）
- 打包為獨立執行檔
- 設定檔或使用者介面
- 多語言支援（僅支援繁體中文）
- OpenCC 繁簡轉換（先觀察 Whisper + prompt 引導的效果）

---

## 6. 技術考量

- **統一使用 PyTorch 作為底層推理引擎**：Whisper 語音辨識和 LLM 文字處理都透過 PyTorch + transformers 執行，避免混用多個推理引擎的相容性問題
- **前提條件**：使用者需有 NVIDIA GPU 並安裝 CUDA
- **RTX 5090 (Blackwell) 相容性**：需使用 PyTorch cu130 wheel（已在 comfyui-civitai-alchemist 專案驗證可行）
- 所有模型首次執行時從 HuggingFace 自動下載
- 整體為**單一 Python 檔案**，透過 uv 管理依賴

---

## 7. 高階驗收標準

- **AC-1**：程式正常啟動並提示使用者操作
- **AC-2**：按 Enter 開始錄音，說一段 5-10 秒的繁體中文，再按 Enter 結束，能在 3 秒內看到辨識結果
- **AC-3**：說話時故意加入贅字（如「嗯...那個...就是說...」），LLM 處理後的結果中不包含這些贅字
- **AC-4**：口述列舉內容（如「第一 A，第二 B，第三 C」），輸出自動格式化為編號列表
- **AC-5**：在一段錄音中談論兩個以上不同主題，輸出有適當的段落分隔
- **AC-6**：LLM 處理後的結果保持原始語意不變，沒有被添加或改寫
- **AC-7**：終端機顯示各階段的處理時間統計

---

## 8. 開放問題

- **LLM 模型選擇**：Qwen3-1.7B 的贅字清理和格式化效果是否足夠好？是否需要更大的模型（如 4B）？這需要透過 MVP 實測來決定
- **Whisper 繁中品質**：large-v3-turbo + prompt 引導是否足以產出穩定的繁體中文？還是需要使用微調模型或 OpenCC 後處理？
- **LLM Prompt 調優**：贅字清理和格式化是否能用同一個 prompt 處理，還是需要分兩次呼叫？
- **長段錄音**：超過 30 秒的錄音（Whisper 的單段上限）該如何處理？MVP 階段是否需要考慮？

---

## 9. 參考資料

- [Typeness 技術可行性研究](../research/2026-02-12-voice-input-tool-feasibility.md)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [Whisper large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo)
- [Qwen3 官方部落格](https://qwenlm.github.io/blog/qwen3/)
- [PyTorch CUDA wheels](https://download.pytorch.org/whl/)
